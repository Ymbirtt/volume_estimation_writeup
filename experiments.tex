\section{Mixing time}\label{sec_mix}

Our first experiment will attempt to bound the mixing time of the Hit and Run random walk in a convex shape. The hit and run walk is known to mix rapidly %cite hit and run is fast and fun, as well as Vempala's proof of warm start-ness
in the sense that the number of steps needed for the walk to reach a uniform ditribution is $O^{*}(n^2)$, however this result actually shows that the walk can only be guaranteed to mix after over $10^{10} n^2$
steps. A hit and run step requires the generation of $n$ uniformly distributed random numbers, which is already a difficult task. Performing $10^{10} n^2$ steps, then, is very much infeasible on modern hardware.

We shall judge the walk to be well-mixed if 1,000 independent points generated by the walk with a fixed starting point at the origin pass a $\chi^2$ test at the 5\% level against the null hypothesis that the points are identically distributed to a set of 1,000 points sampled by a method guaranteed to generate uniform points, but which may be somewhat slower. It is not clear which shape is likely to constitute a pathological case for the hit and run walk; the ice cream has the smallest volume, and so in a sense fewer points need to be chosen from, but also has the tighest possible corner, in which the walk is most likely to get stuck. By contrast, the ball has the largest volume and no corners. We will test both shapes. Uniformly distributed random points can be generated from a ball fairly easily, and uniform random points can be generated from an ice cream by simple rejection sampling. To keep our search reasonably short,  we will restrict ourselves to only testing numbers of steps equal to natural powers of 2.

%Actual write up of results goes here as soon as I can de-dumb the chi squared tests. Kolmogorov-Smirnov tests may have to 

From this, we can see that the number of steps needed to generate a uniform random point is still reasonably large, but nowhere near the upper bound of $10^10$. This is relieving, and we will use these numbers of steps for each dimension in the sequel.


\section{Threads per stage}\label{sec_error}

We will now see how the number of threads per phase influences the accuracy of an estimate. In the paper describing our particular estimation method, it is stated that, for an n-dimensional shape, in order to achieve an error of $\varepsilon$, we should use $400\frac{n\log n}{\varepsilon^2}$ threads per stage. This, again, is an upper bound, and we would like to show that it is loose. We shall vary the number of threads used per stage, and see how the distribution of estimates varies with this value. As before, we will use both the ice cream and the ball to respectively maximise and minimise the ratios between successive shapes. Our analysis here will be largely visual, since any returned value from this algorithm might be considered valid for a sufficiently large value of epsilon.


\section{Computation Time for Convex Shapes}\label{sec_time}

It's now time to test the practicalities of this algorithm on a series of convex shape, against already established methods of exact volume computation. The code used for comparison is the same as that used in %cite exact methods
and was run over a series of randomly generated convex shapes. Each shape is the convex hull of the box $[-1,1]^n$, and some variable number of points on the ball of radius $\sqrt n$. We will randomly generate series of shapes in each dimension, increasing the number of points on the hull, and seeing how this affects the time for a volume estimation to complete to a precision of $\varepsilon = 0.1$.

All experiments were conducted on the same hardware and in the same operating system, using the same time metric. The processor is an Intel i5-2500k in a Gigabyte Z68AP-D3 motherboard, connected to 8 GiB of 1,600 MHz dual-challen DDR3 RAM. The programs were both compiled onto a 64-bit install of Ubuntu 12.04 virtualised by Oracle VM Virtualbox with access to 2GiB of RAM. Each time is measured in elapsed clock cycles divided by the number of clocks per second. Using this processor time removes the possibility of error due to other running processes. Neither program is designed to exploit any more than a single processor core.

The oracle is implemented with a linear programming solver, which tests whether a queried point can be written as a convex combination of the points on a hull. The LP solver used is Gurobi, which is the fastest LP solver currently available.


Unfortunately, despite the high-speed LP solver, the oracle queries are very much not free. The majority of our time in this case is spent answering oracle queries, not performing the auxillary operations. %Talk about scaling of answering oracle queries - if they scale as badly as solving LPs, we're basically wasting our time here. Otherwise, eventually this will be badass